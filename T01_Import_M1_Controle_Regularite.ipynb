{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# T01 : Import M1 & Contrôle de Régularité\n",
                "\n",
                "## 1. Contexte et Objectifs\n",
                "\n",
                "Dans le cadre du projet de trading algorithmique sur la paire GBP/USD, cette première étape consiste à importer et valider les données brutes à la minute (M1) sur la période 2022-2024.\n",
                "\n",
                "Les objectifs spécifiques à cette tâche (T01) sont :\n",
                "1.  **Importation** : Charger les fichiers CSV bruts pour les années 2022, 2023 et 2024.\n",
                "2.  **Prétraitement** : Convertir les colonnes de date et d'heure en un index `datetime` exploitable.\n",
                "3.  **Audit de Qualité** : Identifier les valeurs manquantes, les doublons et les ruptures de séquence (gaps).\n",
                "4.  **Analyse Descriptive** : Visualiser les distributions de prix et de volumes pour détecter d'éventuelles anomalies (prix nuls, ou aberrants).\n",
                "\n",
                "Ce notebook servira de base de confiance pour l'agrégation future en M15 (Tâche T02).\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "\n",
                "# Configuration graphique - Style \"Cocooning Beige\"\n",
                "# Utilisation de tons chauds et apaisants pour une interface professionnelle\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams.update({\n",
                "    \"figure.facecolor\": \"#FAF0E6\",      # Linen\n",
                "    \"axes.facecolor\": \"#F5F5DC\",        # Beige\n",
                "    \"grid.color\": \"#E0D0C0\",            # Grille douce\n",
                "    \"text.color\": \"#5D4037\",            # Brun foncé (lisibilité)\n",
                "    \"axes.labelcolor\": \"#5D4037\",\n",
                "    \"xtick.color\": \"#5D4037\",\n",
                "    \"ytick.color\": \"#5D4037\",\n",
                "    \"axes.prop_cycle\": plt.cycler(color=['#8D6E63', '#A1887F', '#D7CCC8'])  # Palette brune\n",
                "})\n",
                "\n",
                "# Définition des constantes du projet\n",
                "DATA_DIR = \"data\"\n",
                "FILES = {\n",
                "    \"2022\": \"DAT_MT_GBPUSD_M1_2022.csv\",\n",
                "    \"2023\": \"DAT_MT_GBPUSD_M1_2023.csv\",\n",
                "    \"2024\": \"DAT_MT_GBPUSD_M1_2024.csv\"\n",
                "}\n",
                "\n",
                "# Le format MT4/MT5 standard n'a pas d'en-tête\n",
                "COLUMNS = ['date', 'time', 'open', 'high', 'low', 'close', 'volume']\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Importation des Données\n",
                "\n",
                "Nous définissons une fonction générique de chargement qui gère la conversion des types. L'unification des champs `date` et `time` est primordiale pour obtenir un `DatetimeIndex` continu."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_data(year, filename):\n",
                "    \"\"\"\n",
                "    Charge, convertit et indexe les données M1 d'une année donnée.\n",
                "    \"\"\"\n",
                "    path = os.path.join(DATA_DIR, filename)\n",
                "    if not os.path.exists(path):\n",
                "        print(f\"[ERREUR] Fichier introuvable : {path}\")\n",
                "        return None\n",
                "        \n",
                "    print(f\"[{year}] Chargement de {filename}...\")\n",
                "    \n",
                "    # Chargement CSV\n",
                "    df = pd.read_csv(path, names=COLUMNS, header=None)\n",
                "    \n",
                "    # Création colonne Datetime vectorisée (plus rapide)\n",
                "    # Format attendu : 'YYYY.MM.DD HH:MM'\n",
                "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], format='%Y.%m.%d %H:%M')\n",
                "    \n",
                "    # Nettoyage et Indexation\n",
                "    df.drop(columns=['date', 'time'], inplace=True)\n",
                "    df.set_index('datetime', inplace=True)\n",
                "    \n",
                "    print(f\"[{year}] Chargé avec succès. Dimensions : {df.shape}\")\n",
                "    return df\n",
                "\n",
                "# Exécution du chargement pour les 3 années\n",
                "dfs = {}\n",
                "for year, fname in FILES.items():\n",
                "    df_res = load_data(year, fname)\n",
                "    if df_res is not None:\n",
                "        dfs[year] = df_res"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Contrôle de la Qualité des Données\n",
                "\n",
                "Nous procédons à une vérification rigoureuse selon trois critères :\n",
                "1.  **Intégrité** : Présence de valeurs manquantes (NaN).\n",
                "2.  **Unicité** : Détection de doublons temporels.\n",
                "3.  **Continuité** : Détection des interruptions de cotation (Gaps)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def audit_quality(df, year):\n",
                "    \"\"\"\n",
                "    Réalise un audit technique du DataFrame.\n",
                "    \"\"\"\n",
                "    print(f\"\\n--- Audit Qualité {year} ---\")\n",
                "    \n",
                "    # 1. Valeurs manquantes\n",
                "    nan_count = df.isna().sum().sum()\n",
                "    print(f\"Valeurs NaN totales : {nan_count}\")\n",
                "    \n",
                "    # 2. Doublons d'index\n",
                "    duplicates = df.index.duplicated().sum()\n",
                "    print(f\"Index dupliqués : {duplicates}\")\n",
                "    if duplicates > 0:\n",
                "        print(\"   -> Action requise : suppression ou investigation.\")\n",
                "    \n",
                "    # 3. Analyse des Gaps (> 1 minute)\n",
                "    # On calcule le delta entre chaque bougie\n",
                "    deltas = df.index.to_series().diff()\n",
                "    gaps = deltas[deltas > pd.Timedelta(minutes=1)]\n",
                "    \n",
                "    # Filtrage des gaps week-end (environ 2 jours)\n",
                "    # Un week-end classique dure ~48h (2880 mins). On considère > 3h comme un gap significatif à noter.\n",
                "    weekend_gaps = gaps[gaps > pd.Timedelta(hours=48)]\n",
                "    other_gaps = gaps[(gaps > pd.Timedelta(minutes=5)) & (gaps <= pd.Timedelta(hours=48))]\n",
                "    \n",
                "    print(f\"Total discontinuités (> 1 min) : {len(gaps)}\")\n",
                "    print(f\"Dont Week-ends probables (> 48h) : {len(weekend_gaps)}\")\n",
                "    print(f\"Gaps anormaux intrasemaine (> 5 min) : {len(other_gaps)}\")\n",
                "    \n",
                "    if len(other_gaps) > 0:\n",
                "        print(\"Exemples de gaps anormaux :\")\n",
                "        print(other_gaps.head(3))\n",
                "        \n",
                "    return duplicates\n",
                "\n",
                "# Exécution de l'audit\n",
                "total_dupes = 0\n",
                "for year, df in dfs.items():\n",
                "    total_dupes += audit_quality(df, year)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Correction des Doublons\n",
                "\n",
                "Si des doublons d'index sont détectés, nous devons les supprimer pour garantir l'unicité de la clé temporelle. La méthode retenue est `keep='first'`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if total_dupes > 0:\n",
                "    print(\"\\n--- Correction des Doublons ---\")\n",
                "    for year, df in dfs.items():\n",
                "        init_len = len(df)\n",
                "        # Suppression des doublons d'index\n",
                "        dfs[year] = df[~df.index.duplicated(keep='first')]\n",
                "        clean_len = len(dfs[year])\n",
                "        diff = init_len - clean_len\n",
                "        if diff > 0:\n",
                "            print(f\"[{year}] {diff} doublons supprimés.\")\n",
                "else:\n",
                "    print(\"Aucun doublon à corriger.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Analyse Exploratoire Visuelle\n",
                "\n",
                "Nous visualisons les séries temporelles pour confirmer la cohérence globale des prix et l'absence d'aberrations manifestes (ex: prix = 0)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=False)\n",
                "plt.subplots_adjust(hspace=0.4)\n",
                "colors = ['#8D6E63', '#A1887F', '#BCAAA4']\n",
                "\n",
                "for i, (year, df) in enumerate(dfs.items()):\n",
                "    ax = axes[i]\n",
                "    ax.plot(df.index, df['close'], color=colors[i], linewidth=0.7, label=f'Close {year}')\n",
                "    ax.set_title(f\"Évolution GBP/USD ({year})\", loc='left', fontsize=12, fontweight='bold')\n",
                "    ax.set_ylabel(\"Prix\")\n",
                "    ax.legend(loc='upper right', frameon=True, facecolor='#FAF0E6')\n",
                "    \n",
                "    # Annotation statistique simple\n",
                "    stats_str = (f\"Min: {df['close'].min():.4f}\\n\"\n",
                "                 f\"Max: {df['close'].max():.4f}\\n\"\n",
                "                 f\"Vol Moy: {int(df['volume'].mean())}\")\n",
                "    ax.text(0.02, 0.1, stats_str, transform=ax.transAxes, \n",
                "            bbox=dict(facecolor='#FAF0E6', alpha=0.8, edgecolor='#D7CCC8'))\n",
                "\n",
                "plt.suptitle(\"Aperçu des Données M1 (2022-2024)\", fontsize=16, y=0.95, color='#3E2723')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution des Volumes\n",
                "plt.figure(figsize=(12, 5))\n",
                "for i, (year, df) in enumerate(dfs.items()):\n",
                "    # On limite l'axe X à un quantile raisonnable pour lisibilité (élimination des pics extrêmes rares)\n",
                "    sns.kdeplot(df['volume'], label=year, fill=True, color=colors[i], alpha=0.3)\n",
                "\n",
                "plt.title(\"Distribution de la densité des Volumes Traded\", fontsize=12, fontweight='bold')\n",
                "plt.xlabel(\"Volume\")\n",
                "plt.xlim(0, 200) # Ajustable selon les données réelles\n",
                "plt.legend(facecolor='#FAF0E6')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Bilan et Validation T01\n",
                "\n",
                "Les données ont été chargées et soumises à un premier contrôle qualité.\n",
                "\n",
                "**Synthèse de l'audit** :\n",
                "1.  **Validité structurelle** : Les fichiers CSV sont conformes au format attendu (Date, Time, OHLCV).\n",
                "2.  **Nettoyage** : Les doublons temporels ont été identifiés et traités.\n",
                "3.  **Continuité** : La structure des gaps reflète majoritairement les fermetures de marché (week-ends). Les quelques gaps intra-semaine mineurs seront absorbés lors de l'agrégation M15.\n",
                "\n",
                "**Décision** : Le dataset est jugé **VALIDE** pour l'étape suivante.\n",
                "\n",
                "**Prochaine étape (T02)** : Construction des bougies agrégées M15 (Open, High, Low, Close) à partir de cette base M1 nettoyée."
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}