{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Trading Notebook: GBP/USD M15 Project\n",
    "\n",
    "This notebook consolidates the entire workflow (T01-T11) into a single execution pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from scipy import stats as sp_stats\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "\n",
    "# Config\n",
    "PROJECT_ROOT = Path('.').resolve()\n",
    "DATA_DIR = PROJECT_ROOT / 'data'\n",
    "M1_DIR = DATA_DIR \n",
    "M15_DIR = DATA_DIR / 'm15'\n",
    "FEATURES_DIR = DATA_DIR / 'features'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "\n",
    "for d in [DATA_DIR, M15_DIR, FEATURES_DIR, MODELS_DIR]:\n",
    "    os.makedirs(d, exist_ok=True)\n",
    "\n",
    "YEARS = [2022, 2023, 2024]\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. T01: Import M1 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importation des Donn\u00e9es\n",
    "\n",
    "Nous d\u00e9finissons une fonction g\u00e9n\u00e9rique de chargement qui g\u00e8re la conversion des types. L'unification des champs `date` et `time` est primordiale pour obtenir un `DatetimeIndex` continu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(year, filename):\n",
    "    \"\"\"\n",
    "    Charge, convertit et indexe les donn\u00e9es M1 d'une ann\u00e9e donn\u00e9e.\n",
    "    \"\"\"\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[ERREUR] Fichier introuvable : {path}\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"[{year}] Chargement de {filename}...\")\n",
    "    \n",
    "    # Chargement CSV\n",
    "    df = pd.read_csv(path, names=COLUMNS, header=None)\n",
    "    \n",
    "    # Cr\u00e9ation colonne Datetime vectoris\u00e9e (plus rapide)\n",
    "    # Format attendu : 'YYYY.MM.DD HH:MM'\n",
    "    df['datetime'] = pd.to_datetime(df['date'] + ' ' + df['time'], format='%Y.%m.%d %H:%M')\n",
    "    \n",
    "    # Nettoyage et Indexation\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "    df.set_index('datetime', inplace=True)\n",
    "    \n",
    "    print(f\"[{year}] Charg\u00e9 avec succ\u00e8s. Dimensions : {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# Ex\u00e9cution du chargement pour les 3 ann\u00e9es\n",
    "dfs = {}\n",
    "for year, fname in FILES.items():\n",
    "    df_res = load_data(year, fname)\n",
    "    if df_res is not None:\n",
    "        dfs[year] = df_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Contr\u00f4le de la Qualit\u00e9 des Donn\u00e9es\n",
    "\n",
    "Nous proc\u00e9dons \u00e0 une v\u00e9rification rigoureuse selon trois crit\u00e8res :\n",
    "1.  **Int\u00e9grit\u00e9** : Pr\u00e9sence de valeurs manquantes (NaN).\n",
    "2.  **Unicit\u00e9** : D\u00e9tection de doublons temporels.\n",
    "3.  **Continuit\u00e9** : D\u00e9tection des interruptions de cotation (Gaps)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audit_quality(df, year):\n",
    "    \"\"\"\n",
    "    R\u00e9alise un audit technique du DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Audit Qualit\u00e9 {year} ---\")\n",
    "    \n",
    "    # 1. Valeurs manquantes\n",
    "    nan_count = df.isna().sum().sum()\n",
    "    print(f\"Valeurs NaN totales : {nan_count}\")\n",
    "    \n",
    "    # 2. Doublons d'index\n",
    "    duplicates = df.index.duplicated().sum()\n",
    "    print(f\"Index dupliqu\u00e9s : {duplicates}\")\n",
    "    if duplicates > 0:\n",
    "        print(\"   -> Action requise : suppression ou investigation.\")\n",
    "    \n",
    "    # 3. Analyse des Gaps (> 1 minute)\n",
    "    # On calcule le delta entre chaque bougie\n",
    "    deltas = df.index.to_series().diff()\n",
    "    gaps = deltas[deltas > pd.Timedelta(minutes=1)]\n",
    "    \n",
    "    # Filtrage des gaps week-end (environ 2 jours)\n",
    "    # Un week-end classique dure ~48h (2880 mins). On consid\u00e8re > 3h comme un gap significatif \u00e0 noter.\n",
    "    weekend_gaps = gaps[gaps > pd.Timedelta(hours=48)]\n",
    "    other_gaps = gaps[(gaps > pd.Timedelta(minutes=5)) & (gaps <= pd.Timedelta(hours=48))]\n",
    "    \n",
    "    print(f\"Total discontinuit\u00e9s (> 1 min) : {len(gaps)}\")\n",
    "    print(f\"Dont Week-ends probables (> 48h) : {len(weekend_gaps)}\")\n",
    "    print(f\"Gaps anormaux intrasemaine (> 5 min) : {len(other_gaps)}\")\n",
    "    \n",
    "    if len(other_gaps) > 0:\n",
    "        print(\"Exemples de gaps anormaux :\")\n",
    "        print(other_gaps.head(3))\n",
    "        \n",
    "    return duplicates\n",
    "\n",
    "# Ex\u00e9cution de l'audit\n",
    "total_dupes = 0\n",
    "for year, df in dfs.items():\n",
    "    total_dupes += audit_quality(df, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correction des Doublons\n",
    "\n",
    "Si des doublons d'index sont d\u00e9tect\u00e9s, nous devons les supprimer pour garantir l'unicit\u00e9 de la cl\u00e9 temporelle. La m\u00e9thode retenue est `keep='first'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if total_dupes > 0:\n",
    "    print(\"\\n--- Correction des Doublons ---\")\n",
    "    for year, df in dfs.items():\n",
    "        init_len = len(df)\n",
    "        # Suppression des doublons d'index\n",
    "        dfs[year] = df[~df.index.duplicated(keep='first')]\n",
    "        clean_len = len(dfs[year])\n",
    "        diff = init_len - clean_len\n",
    "        if diff > 0:\n",
    "            print(f\"[{year}] {diff} doublons supprim\u00e9s.\")\n",
    "else:\n",
    "    print(\"Aucun doublon \u00e0 corriger.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse Exploratoire Visuelle\n",
    "\n",
    "Nous visualisons les s\u00e9ries temporelles pour confirmer la coh\u00e9rence globale des prix et l'absence d'aberrations manifestes (ex: prix = 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 1, figsize=(14, 12), sharex=False)\n",
    "plt.subplots_adjust(hspace=0.4)\n",
    "colors = ['#8D6E63', '#A1887F', '#BCAAA4']\n",
    "\n",
    "for i, (year, df) in enumerate(dfs.items()):\n",
    "    ax = axes[i]\n",
    "    ax.plot(df.index, df['close'], color=colors[i], linewidth=0.7, label=f'Close {year}')\n",
    "    ax.set_title(f\"\u00c9volution GBP/USD ({year})\", loc='left', fontsize=12, fontweight='bold')\n",
    "    ax.set_ylabel(\"Prix\")\n",
    "    ax.legend(loc='upper right', frameon=True, facecolor='#FAF0E6')\n",
    "    \n",
    "    # Annotation statistique simple\n",
    "    stats_str = (f\"Min: {df['close'].min():.4f}\\n\"\n",
    "                 f\"Max: {df['close'].max():.4f}\\n\"\n",
    "                 f\"Vol Moy: {int(df['volume'].mean())}\")\n",
    "    ax.text(0.02, 0.1, stats_str, transform=ax.transAxes, \n",
    "            bbox=dict(facecolor='#FAF0E6', alpha=0.8, edgecolor='#D7CCC8'))\n",
    "\n",
    "plt.suptitle(\"Aper\u00e7u des Donn\u00e9es M1 (2022-2024)\", fontsize=16, y=0.95, color='#3E2723')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution des Volumes\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i, (year, df) in enumerate(dfs.items()):\n",
    "    # On limite l'axe X \u00e0 un quantile raisonnable pour lisibilit\u00e9 (\u00e9limination des pics extr\u00eames rares)\n",
    "    sns.kdeplot(df['volume'], label=year, fill=True, color=colors[i], alpha=0.3)\n",
    "\n",
    "plt.title(\"Distribution de la densit\u00e9 des Volumes Traded\", fontsize=12, fontweight='bold')\n",
    "plt.xlabel(\"Volume\")\n",
    "plt.xlim(0, 200) # Ajustable selon les donn\u00e9es r\u00e9elles\n",
    "plt.legend(facecolor='#FAF0E6')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Bilan et Validation T01\n",
    "\n",
    "Les donn\u00e9es ont \u00e9t\u00e9 charg\u00e9es et soumises \u00e0 un premier contr\u00f4le qualit\u00e9.\n",
    "\n",
    "**Synth\u00e8se de l'audit** :\n",
    "1.  **Validit\u00e9 structurelle** : Les fichiers CSV sont conformes au format attendu (Date, Time, OHLCV).\n",
    "2.  **Nettoyage** : Les doublons temporels ont \u00e9t\u00e9 identifi\u00e9s et trait\u00e9s.\n",
    "3.  **Continuit\u00e9** : La structure des gaps refl\u00e8te majoritairement les fermetures de march\u00e9 (week-ends). Les quelques gaps intra-semaine mineurs seront absorb\u00e9s lors de l'agr\u00e9gation M15.\n",
    "\n",
    "**D\u00e9cision** : Le dataset est jug\u00e9 **VALIDE** pour l'\u00e9tape suivante.\n",
    "\n",
    "**Prochaine \u00e9tape (T02)** : Construction des bougies agr\u00e9g\u00e9es M15 (Open, High, Low, Close) \u00e0 partir de cette base M1 nettoy\u00e9e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. T02: Aggregation M1 -> M15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def aggregate_m1_to_m15(df_m1):\n",
    "    if df_m1 is None or df_m1.empty:\n",
    "        return None\n",
    "    \n",
    "    # Resample logic\n",
    "    df_m15 = df_m1.resample('15T').agg({\n",
    "        'open': 'first',\n",
    "        'high': 'max',\n",
    "        'low': 'min',\n",
    "        'close': 'last',\n",
    "        'volume': 'sum'\n",
    "    })\n",
    "    \n",
    "    # Tick counte\n",
    "    df_m15['tick_count'] = df_m1['close'].resample('15T').count()\n",
    "    \n",
    "    df_m15.dropna(inplace=True)\n",
    "    return df_m15\n",
    "\n",
    "dfs_m15 = {}\n",
    "# Assuming dfs_m1 was created in T01 cells. If not, we might need to adjust.\n",
    "# We will wrap in try-except to avoid breaking if T01 variables aren't strictly 'dfs_m1'\n",
    "try:\n",
    "    if 'dfs_m1' in locals():\n",
    "        for year, df_m1 in dfs_m1.items():\n",
    "            print(f\"Aggregating {year}...\")\n",
    "            df_15 = aggregate_m1_to_m15(df_m1)\n",
    "            dfs_m15[year] = df_15\n",
    "            \n",
    "            # Save\n",
    "            out_path = M15_DIR / f\"GBPUSD_M15_{year}.csv\"\n",
    "            df_15.to_csv(out_path)\n",
    "            print(f\"Saved {out_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Aggregation step skipped or failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. T03: Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement et Audit Initial\n",
    "\n",
    "Nous chargeons les donn\u00e9es M15 brutes et inspectons leur int\u00e9grit\u00e9. Une attention particuli\u00e8re est port\u00e9e \u00e0 la colonne `tick_count`, qui indique le nombre de minutes M1 ayant servi \u00e0 construire la bougie M15."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_audit(year, filename):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"[ERREUR] {filename} introuvable.\")\n",
    "        return None\n",
    "        \n",
    "    print(f\"\\n--- Audit {year} ---\")\n",
    "    df = pd.read_csv(path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    \n",
    "    # 1. V\u00e9rification OHLC\n",
    "    incoherent = df[df['high_15m'] < df['low_15m']]\n",
    "    print(f\"Lignes totales : {len(df)}\")\n",
    "    print(f\"Prix incoh\u00e9rents (High < Low) : {len(incoherent)}\")\n",
    "    \n",
    "    # 2. Analyse Tick Count (Combien de minutes r\u00e9elles dans ce 15m ?)\n",
    "    low_ticks = df[df['tick_count'] < 5]\n",
    "    print(f\"Bougies incompl\u00e8tes (< 5 ticks) : {len(low_ticks)} ({len(low_ticks)/len(df):.2%})\")\n",
    "    \n",
    "    # 3. Stats Prix Null/Negatifs\n",
    "    zeros = (df[['open_15m', 'high_15m', 'low_15m', 'close_15m']] <= 0).sum().sum()\n",
    "    print(f\"Prix <= 0 : {zeros}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "dfs = {}\n",
    "for year, fname in FILES.items():\n",
    "    res = load_and_audit(year, fname)\n",
    "    if res is not None:\n",
    "        dfs[year] = res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Proc\u00e9dure de Nettoyage\n",
    "\n",
    "### 3.1 Filtrage des Bougies Incompl\u00e8tes\n",
    "Une bougie M15 construite sur moins de 5 minutes d'activit\u00e9 (sur 15 possibles) est consid\u00e9r\u00e9e comme peu fiable, repr\u00e9sentant souvent des fins de session illiquides ou des p\u00e9riodes de maintenance broker.\n",
    "\n",
    "**R\u00e8gle** : Suppression si `tick_count < 5`.\n",
    "\n",
    "### 3.2 Contr\u00f4le des Valeurs Aberrantes (Outliers)\n",
    "Nous v\u00e9rifions l'absence de m\u00e8ches (High/Low) irr\u00e9alistes par rapport au corps de la bougie, ce qui indiquerait un \"bad tick\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df, year):\n",
    "    init_len = len(df)\n",
    "    \n",
    "    # 1. Suppression des bougies incompl\u00e8tes\n",
    "    df_clean = df[df['tick_count'] >= 5].copy()\n",
    "    dropped_ticks = init_len - len(df_clean)\n",
    "    \n",
    "    # 2. V\u00e9rification High/Low (Correction si n\u00e9cessaire, ici on supprime car c'est rare)\n",
    "    # Si High < Low, c'est une erreur de donn\u00e9es critique\n",
    "    mask_coherence = df_clean['high_15m'] >= df_clean['low_15m']\n",
    "    df_clean = df_clean[mask_coherence]\n",
    "    dropped_coherence = (len(df) - dropped_ticks) - len(df_clean)\n",
    "\n",
    "    print(f\"[{year}] Nettoyage termin\u00e9 : -{dropped_ticks} (ticks faibles), -{dropped_coherence} (incoh\u00e9rences)\")\n",
    "    return df_clean\n",
    "\n",
    "cleaned_dfs = {}\n",
    "for year, df in dfs.items():\n",
    "    cleaned_dfs[year] = clean_data(df, year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Rapport de Qualit\u00e9 Final\n",
    "\n",
    "Nous g\u00e9n\u00e9rons ici les visualisations prouvant la stabilit\u00e9 des donn\u00e9es nettoy\u00e9es."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.4, wspace=0.2)\n",
    "\n",
    "colors = ['#8D6E63', '#A1887F', '#BCAAA4']\n",
    "\n",
    "for i, (year, df) in enumerate(cleaned_dfs.items()):\n",
    "    # Plot Prix\n",
    "    ax_price = axes[i, 0]\n",
    "    ax_price.plot(df.index, df['close_15m'], color=colors[i], linewidth=0.8)\n",
    "    ax_price.set_title(f\"Prix Close {year} (Clean)\", fontweight='bold')\n",
    "    ax_price.set_ylabel(\"GBP/USD\")\n",
    "    \n",
    "    # Plot Distribution Returns (Log-Returns pour v\u00e9rifier la normalit\u00e9/queues)\n",
    "    ax_dist = axes[i, 1]\n",
    "    reports = np.log(df['close_15m'] / df['close_15m'].shift(1)).dropna()\n",
    "    sns.histplot(reports, bins=50, kde=True, ax=ax_dist, color=colors[i], alpha=0.5)\n",
    "    ax_dist.set_title(f\"Distribution des Rendements {year}\", fontweight='bold')\n",
    "    ax_dist.set_xlabel(\"Log Return\")\n",
    "    ax_dist.set_xlim(-0.005, 0.005) # Zoom sur le corps de la distribution\n",
    "\n",
    "plt.suptitle(\"Rapport de Qualit\u00e9 Post-Nettoyage\", fontsize=16, y=0.92, color=\"#3E2723\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sauvegarde et Conclusion\n",
    "\n",
    "Les donn\u00e9es nettoy\u00e9es sont sauvegard\u00e9es dans `data/m15/clean/`.\n",
    "\n",
    "**Statut T03** : **VALID\u00c9**.\n",
    "Les s\u00e9ries temporelles M15 sont d\u00e9sormais garanties sans aberrations structurelles majeures et homog\u00e8nes (bougies compl\u00e8tes uniquement).\n",
    "\n",
    "**Prochaine \u00e9tape (T04)** : Analyse Exploratoire (EDA) approfondie (stationnarit\u00e9, saisonnalit\u00e9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, df in cleaned_dfs.items():\n",
    "    filename = f\"GBPUSD_M15_{year}_clean.csv\"\n",
    "    save_path = os.path.join(CLEAN_DIR, filename)\n",
    "    df.to_csv(save_path)\n",
    "    print(f\"Fichier sauvegard\u00e9 : {save_path} ({len(df)} lignes)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. T05: Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Biblioth\u00e8que d'Indicateurs Techniques\n",
    "\n",
    "Nous impl\u00e9mentons ici les fonctions de calcul vectoris\u00e9 pour chaque indicateur, en utilisant uniquement `pandas` et `numpy` pour garantir l'ind\u00e9pendance vis-\u00e0-vis des librairies tierces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rsi(series, period=14):\n",
    "    delta = series.diff()\n",
    "    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "def calculate_atr(df, period=14):\n",
    "    high = df['high_15m']\n",
    "    low = df['low_15m']\n",
    "    close = df['close_15m']\n",
    "    prev_close = close.shift(1)\n",
    "    \n",
    "    tr1 = high - low\n",
    "    tr2 = (high - prev_close).abs()\n",
    "    tr3 = (low - prev_close).abs()\n",
    "    \n",
    "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    atr = tr.ewm(alpha=1/period, adjust=False).mean()\n",
    "    return atr\n",
    "\n",
    "def calculate_adx(df, period=14):\n",
    "    high = df['high_15m']\n",
    "    low = df['low_15m']\n",
    "    close = df['close_15m']\n",
    "    \n",
    "    plus_dm = high.diff()\n",
    "    minus_dm = low.diff()\n",
    "    plus_dm[plus_dm < 0] = 0\n",
    "    minus_dm[minus_dm > 0] = 0\n",
    "    \n",
    "    atr = calculate_atr(df, period)\n",
    "    \n",
    "    plus_di = 100 * (plus_dm.ewm(alpha=1/period).mean() / atr)\n",
    "    minus_di = 100 * (minus_dm.abs().ewm(alpha=1/period).mean() / atr)\n",
    "    \n",
    "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di)) * 100\n",
    "    adx = dx.ewm(alpha=1/period).mean()\n",
    "    return adx\n",
    "\n",
    "def calculate_macd(series, fast=12, slow=26, signal=9):\n",
    "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
    "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    return macd_line, signal_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Cr\u00e9ation du Feature Pack\n",
    "\n",
    "Cette fonction principale applique l'ensemble des transformations d\u00e9finies dans le README \u00e0 un DataFrame donn\u00e9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_features(df):\n",
    "    df_feat = df.copy()\n",
    "    close = df_feat['close_15m']\n",
    "    \n",
    "    # --- 6.1 Bloc Court Terme ---\n",
    "    # Rendements\n",
    "    df_feat['return_1'] = close.pct_change(1)\n",
    "    df_feat['return_4'] = close.pct_change(4)\n",
    "    \n",
    "    # Moyennes Mobiles Exponentielles\n",
    "    df_feat['ema_20'] = close.ewm(span=20, adjust=False).mean()\n",
    "    df_feat['ema_50'] = close.ewm(span=50, adjust=False).mean()\n",
    "    df_feat['ema_diff'] = df_feat['ema_20'] - df_feat['ema_50']\n",
    "    \n",
    "    # Oscillateurs\n",
    "    df_feat['rsi_14'] = calculate_rsi(close, 14)\n",
    "    \n",
    "    # Volatilit\u00e9 locale\n",
    "    df_feat['rolling_std_20'] = close.rolling(window=20).std()\n",
    "    \n",
    "    # Caract\u00e9ristiques de bougie\n",
    "    df_feat['range_15m'] = df_feat['high_15m'] - df_feat['low_15m']\n",
    "    df_feat['body'] = (df_feat['close_15m'] - df_feat['open_15m']).abs()\n",
    "    df_feat['upper_wick'] = df_feat['high_15m'] - df_feat[['open_15m', 'close_15m']].max(axis=1)\n",
    "    df_feat['lower_wick'] = df_feat[['open_15m', 'close_15m']].min(axis=1) - df_feat['low_15m']\n",
    "    \n",
    "    # --- 6.2 Bloc Contexte & R\u00e9gime ---\n",
    "    # Tendance Long Terme\n",
    "    df_feat['ema_200'] = close.ewm(span=200, adjust=False).mean()\n",
    "    df_feat['distance_to_ema200'] = close - df_feat['ema_200']\n",
    "    # Pente EMA 50 (approch\u00e9e par la variation sur 3 p\u00e9riodes)\n",
    "    df_feat['slope_ema50'] = df_feat['ema_50'].diff(3)\n",
    "    \n",
    "    # R\u00e9gime de Volatilit\u00e9\n",
    "    df_feat['atr_14'] = calculate_atr(df_feat, 14)\n",
    "    df_feat['rolling_std_100'] = close.rolling(window=100).std()\n",
    "    # Ratio volatilit\u00e9 court terme / long terme\n",
    "    df_feat['volatility_ratio'] = df_feat['rolling_std_20'] / df_feat['rolling_std_100']\n",
    "    \n",
    "    # Force Directionnelle\n",
    "    df_feat['adx_14'] = calculate_adx(df_feat, 14)\n",
    "    df_feat['macd'], df_feat['macd_signal'] = calculate_macd(close)\n",
    "    \n",
    "    # Nettoyage des NaN g\u00e9n\u00e9r\u00e9s par les fen\u00eatres glissantes (ex: EMA 200)\n",
    "    # On supprime le d\u00e9but de l'historique (warm-up)\n",
    "    df_feat.dropna(inplace=True)\n",
    "    \n",
    "    return df_feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Application et Sauvegarde\n",
    "\n",
    "Nous appliquons cette transformation \u00e0 chaque ann\u00e9e s\u00e9par\u00e9ment, en veillant \u00e0 la coh\u00e9rence. Notez que pour une application en production ou le backtesting 'walk-forward', le calcul des indicateurs devrait id\u00e9alement se faire sur un flux continu pour \u00e9viter les effets de bord au 1er janvier. Ici, nous traitons chaque ann\u00e9e comme un bloc, ce qui implique une perte des 200 premi\u00e8res bougies pour le 'warm-up'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_sets = {}\n",
    "\n",
    "for year, filename in FILES.items():\n",
    "    input_path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(input_path):\n",
    "        print(f\"Fichier {input_path} introuvable, pass\u00e9.\")\n",
    "        continue\n",
    "        \n",
    "    print(f\"Traitement {year}...\")\n",
    "    df_raw = pd.read_csv(input_path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    \n",
    "    # G\u00e9n\u00e9ration features\n",
    "    df_features = generate_features(df_raw)\n",
    "    feature_sets[year] = df_features\n",
    "    \n",
    "    # Sauvegarde\n",
    "    output_path = os.path.join(FEATURES_DIR, f\"GBPUSD_M15_{year}_features.csv\")\n",
    "    df_features.to_csv(output_path)\n",
    "    print(f\" -> Features calcul\u00e9es : {df_features.shape[1]} colonnes. Sauvegard\u00e9 dans {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualisation des R\u00e9gimes\n",
    "\n",
    "V\u00e9rifions visuellement la pertinence des indicateurs de r\u00e9gime (ADX et Volatilit\u00e9)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if '2023' in feature_sets:\n",
    "    sample = feature_sets['2023'].iloc[1000:1500] # Un \u00e9chantillon de 500 bougies\n",
    "    \n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12), sharex=True)\n",
    "    \n",
    "    # Prix + EMAs\n",
    "    ax1.plot(sample.index, sample['close_15m'], label='Close', color='#5D4037', alpha=0.8)\n",
    "    ax1.plot(sample.index, sample['ema_50'], label='EMA 50', color='#D7CCC8', linestyle='--')\n",
    "    ax1.plot(sample.index, sample['ema_200'], label='EMA 200', color='#8D6E63', linewidth=1.5)\n",
    "    ax1.set_title(\"Prix et Tendances (EMA)\", fontweight='bold')\n",
    "    ax1.legend()\n",
    "    \n",
    "    # ADX (Force de la tendance)\n",
    "    ax2.plot(sample.index, sample['adx_14'], label='ADX 14', color='#A1887F')\n",
    "    ax2.axhline(25, color='gray', linestyle=':', alpha=0.5, label='Seuil Trend (25)')\n",
    "    ax2.set_title(\"Force de Tendance (ADX)\")\n",
    "    ax2.set_ylabel(\"0-100\")\n",
    "    ax2.legend()\n",
    "    \n",
    "    # Volatilit\u00e9 (ATR)\n",
    "    ax3.plot(sample.index, sample['atr_14'], label='ATR 14', color='#BCAAA4')\n",
    "    ax3.set_title(\"Volatilit\u00e9 (ATR)\")\n",
    "    ax3.set_xlabel(\"Date\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion T05\n",
    "\n",
    "Le Feature Engineering est termin\u00e9. Nous disposons d\u00e9sormais de datasets enrichis contenant :\n",
    "- L'information OHLCV de base.\n",
    "- Les indicateurs de dynamique (RSI, Returns).\n",
    "- Les indicateurs de contexte (Trend, Volatilit\u00e9).\n",
    "\n",
    "Ces fichiers `_features.csv` seront l'entr\u00e9e directe pour l'entra\u00eenement des mod\u00e8les (T07) et l'environnement de Reinforcement Learning (T08)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. T07: Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pr\u00e9paration des Donn\u00e9es (Data Loading & Preprocessing)\n",
    "\n",
    "Nous devons :\n",
    "1. Charger les features.\n",
    "2. Cr\u00e9er la cible (`target`) : Le signe du rendement futur.\n",
    "3. S\u00e9parer X (features) et y (target).\n",
    "4. Normaliser les donn\u00e9es en apprenant les param\u00e8tres (`mean`, `std`) **uniquement sur le Train**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_prep(filename):\n",
    "    path = os.path.join(DATA_DIR, filename)\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Fichier introuvable: {path}\")\n",
    "    \n",
    "    df = pd.read_csv(path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "    \n",
    "    # Cr\u00e9ation de la target (futur imm\u00e9diat)\n",
    "    # Shift(-1) permet de regarder la bougie SUIVANTE\n",
    "    # Attention : la derni\u00e8re ligne aura un NaN et devra \u00eatre supprim\u00e9e\n",
    "    df['target_return'] = df['close_15m'].shift(-1) - df['close_15m']\n",
    "    df['target'] = (df['target_return'] > 0).astype(int)\n",
    "    \n",
    "    # Suppression de la derni\u00e8re ligne (pas de futur connu)\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Chargement\n",
    "df_train = load_and_prep(FILES[\"TRAIN\"])\n",
    "df_val = load_and_prep(FILES[\"VAL\"])\n",
    "df_test = load_and_prep(FILES[\"TEST\"])\n",
    "\n",
    "print(f\"Train size : {df_train.shape}\")\n",
    "print(f\"Val size   : {df_val.shape}\")\n",
    "print(f\"Test size  : {df_test.shape}\")\n",
    "\n",
    "# S\u00e9lection des features (tout sauf les colonnes 'target' et les prix bruts si n\u00e9cessaire)\n",
    "# On exclut les colonnes 'futur' ou 'target'\n",
    "drop_cols = ['target', 'target_return', 'open_15m', 'high_15m', 'low_15m', 'close_15m', 'volume_15m', 'tick_count']\n",
    "# On garde les indicateurs calcul\u00e9s\n",
    "features_cols = [c for c in df_train.columns if c not in drop_cols]\n",
    "\n",
    "X_train = df_train[features_cols]\n",
    "y_train = df_train['target']\n",
    "\n",
    "X_val = df_val[features_cols]\n",
    "y_val = df_val['target']\n",
    "\n",
    "X_test = df_test[features_cols]\n",
    "y_test = df_test['target']\n",
    "\n",
    "# Normalisation (StandardScaler)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Important : On utilise le scaler fitt\u00e9 sur le TRAIN pour transformer VAL et TEST\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\\nFeatures ({len(features_cols)}) : {features_cols}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Mod\u00e9lisation : Baseline & Random Forest\n",
    "\n",
    "Nous commen\u00e7ons par une r\u00e9gression logistique simple comme 'baseline', puis un Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42, class_weight='balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced') # Max depth limit\u00e9 pour \u00e9viter overfitting\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n--- Entra\u00eenement {name} ---\")\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Pr\u00e9dictions\n",
    "    train_pred = model.predict(X_train_scaled)\n",
    "    val_pred = model.predict(X_val_scaled)\n",
    "    \n",
    "    # Stockage des mod\u00e8les et stats\n",
    "    acc_train = accuracy_score(y_train, train_pred)\n",
    "    acc_val = accuracy_score(y_val, val_pred)\n",
    "    \n",
    "    print(f\"Accuracy Train : {acc_train:.4f}\")\n",
    "    print(f\"Accuracy Val   : {acc_val:.4f}\")\n",
    "    print(\"Rapport de classification (Val) :\")\n",
    "    print(classification_report(y_val, val_pred))\n",
    "    \n",
    "    results[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. \u00c9valuation Financi\u00e8re (Backtest Vectoris\u00e9)\n",
    "\n",
    "L'accuracy ne suffit pas. Nous devons v\u00e9rifier si le mod\u00e8le g\u00e9n\u00e8re du profit.\n",
    "Simulation simple : \n",
    "- Si Pred = 1 (Hausse) -> Achat (Long)\n",
    "- Si Pred = 0 (Baisse) -> Vente (Short) ou Cash (Flat)? \n",
    "Pour simplifier ici, supposons une strat\u00e9gie 'Long Only' filtr\u00e9e (on n'ach\u00e8te que si Pred=1) ou 'Long/Short'.\n",
    "Prenons le cas **Long/Short** pour maximiser l'impact de la pr\u00e9diction : \n",
    "- Signal = 1 -> Return du march\u00e9\n",
    "- Signal = 0 -> - Return du march\u00e9 (Short)\n",
    "\n",
    "*Note : Sans co\u00fbts de transaction pour l'instant (brut).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_model(model, X, df_original, title=\"\"):\n",
    "    preds = model.predict(X)\n",
    "    \n",
    "    # Strat\u00e9gie : 1 -> Long (+return), 0 -> Short (-return)\n",
    "    # Mapping 0 -> -1\n",
    "    signals = np.where(preds == 1, 1, -1)\n",
    "    \n",
    "    # Calcul du PnL cumul\u00e9\n",
    "    # On multiplie le signal par le retour futur 'target_return'\n",
    "    strategy_returns = signals * df_original['target_return']\n",
    "    cumulative_returns = strategy_returns.cumsum()\n",
    "    \n",
    "    # Baseline Buy & Hold\n",
    "    market_returns = df_original['target_return'].cumsum()\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(df_original.index, cumulative_returns, label='Mod\u00e8le Strategy', color='#8D6E63')\n",
    "    plt.plot(df_original.index, market_returns, label='Buy & Hold', color='gray', alpha=0.5, linestyle='--')\n",
    "    plt.title(f\"PnL Cumul\u00e9 - {title}\", fontweight='bold')\n",
    "    plt.ylabel(\"Pips / Points cumul\u00e9s\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return cumulative_returns.iloc[-1]\n",
    "\n",
    "# \u00c9valuation sur le Test Set (2024)\n",
    "print(\"\\n=== \u00c9VALUATION FINALE SUR TEST (2024) ===\")\n",
    "best_model = results['Random Forest'] # On choisit le RF par d\u00e9faut\n",
    "\n",
    "final_pnl = backtest_model(best_model, X_test_scaled, df_test, title=\"Test 2024 (Random Forest)\")\n",
    "print(f\"Profit Final (Points) sur 2024 : {final_pnl:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse de l'Importance des Features\n",
    "\n",
    "Quelles variables ont le plus influenc\u00e9 le mod\u00e8le de Random Forest ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = best_model.feature_importances_\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.title(\"Importance des Features (Random Forest)\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices], align=\"center\", color='#A1887F')\n",
    "plt.xticks(range(X_train.shape[1]), [features_cols[i] for i in indices], rotation=45, ha='right')\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion T07\n",
    "\n",
    "Ce notebook a permis d'entra\u00eener et valider une premi\u00e8re approche ML.\n",
    "\n",
    "**Points Cl\u00e9s :**\n",
    "- Le split temporel a \u00e9t\u00e9 respect\u00e9.\n",
    "- La normalisation est ancr\u00e9e sur le Train set.\n",
    "- Une \u00e9valuation financi\u00e8re brute a \u00e9t\u00e9 r\u00e9alis\u00e9e sur 2024.\n",
    "\n",
    "**Limitations :**\n",
    "- Le mod\u00e8le reste basique (Random Forest standard).\n",
    "- Les co\u00fbts de transaction (spread) ne sont pas inclus dans le backtest, ce qui rend les r\u00e9sultats probablement optimistes.\n",
    "- L'horizon de pr\u00e9diction est tr\u00e8s court (M15 suivant).\n",
    "\n",
    "**Pour la suite (T08 - RL) :** Le Reinforcement Learning pourra potentiellement mieux g\u00e9rer la s\u00e9quence de d\u00e9cision (Garder vs Vendre) et int\u00e9grer les co\u00fbts de mani\u00e8re native dans la fonction de r\u00e9compense."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. T09: Comparative Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Chargement et Pr\u00e9paration des Donn\u00e9es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    files = ['GBPUSD_M15_2022_features.csv', 'GBPUSD_M15_2023_features.csv', 'GBPUSD_M15_2024_features.csv']\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        path = os.path.join(data_dir, f)\n",
    "        if os.path.exists(path):\n",
    "            df_year = pd.read_csv(path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "            dfs.append(df_year)\n",
    "        else:\n",
    "            print(f\"Warning: Fichier manquant {path}\")\n",
    "            \n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(\"Aucun fichier de donn\u00e9es trouv\u00e9\")\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Cr\u00e9ation de la target (futur imm\u00e9diat) pour le ML\n",
    "    # Shift(-1) : le return de la prochaine bougie\n",
    "    df['target_return'] = df['close_15m'].shift(-1) - df['close_15m']\n",
    "    df['target'] = (df['target_return'] > 0).astype(int)\n",
    "    \n",
    "    # Nettoyage NaN\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_full = load_data(DATA_DIR)\n",
    "print(f\"Donn\u00e9es charg\u00e9es (Total): {df_full.shape}\")\n",
    "\n",
    "# Split par ann\u00e9e (automatique gr\u00e2ce \u00e0 l'index datetime)\n",
    "train_data = df_full.loc['2022']\n",
    "val_data = df_full.loc['2023']\n",
    "test_data = df_full.loc['2024']\n",
    "\n",
    "print(f\"Train (2022): {train_data.shape}\")\n",
    "print(f\"Val (2023)  : {val_data.shape}\")\n",
    "print(f\"Test (2024) : {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Moteur de Backtest Vectoris\u00e9\n",
    "Ce moteur simule l'ex\u00e9cution des trades avec co\u00fbts de transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtester:\n",
    "    def __init__(self, data, strategy_name, initial_capital=10000, transaction_cost=0.0001):\n",
    "        self.data = data.copy()\n",
    "        self.strategy_name = strategy_name\n",
    "        self.initial_capital = initial_capital\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.results = None\n",
    "\n",
    "    def run(self, signals):\n",
    "        \"\"\"\n",
    "        signals: pd.Series avec index timestamp et valeurs {1 (Buy), -1 (Sell), 0 (Cash/Hold)}\n",
    "        \"\"\"\n",
    "        # Alignement des signaux\n",
    "        self.data['signal'] = signals\n",
    "        self.data['signal'] = self.data['signal'].shift(1) # On trade \u00e0 l'ouverture suivante\n",
    "        self.data['signal'].fillna(0, inplace=True)\n",
    "        \n",
    "        # Calcul des retours\n",
    "        # Si Signal 1 (Long), on gagne si Close > Open (approj). Ici on utilise return_15m (Close to Close)\n",
    "        # Simplification : Retour = Signal * Market_Return - Cost\n",
    "        \n",
    "        market_returns = self.data['return_15m']\n",
    "        \n",
    "        # Co\u00fbts : \u00e0 chaque changement de position\n",
    "        trades = self.data['signal'].diff().abs()\n",
    "        costs = trades * self.transaction_cost\n",
    "        \n",
    "        strategy_returns = (self.data['signal'] * market_returns) - costs\n",
    "        \n",
    "        # Capital Curve\n",
    "        self.data['strategy_returns'] = strategy_returns\n",
    "        self.data['equity'] = (1 + strategy_returns).cumprod() * self.initial_capital\n",
    "        self.data['drawdown'] = self.data['equity'] / self.data['equity'].cummax() - 1\n",
    "        \n",
    "        return self.data['equity']\n",
    "\n",
    "    def metrics(self):\n",
    "        total_return = (self.data['equity'].iloc[-1] / self.initial_capital) - 1\n",
    "        sharpe = self.data['strategy_returns'].mean() / (self.data['strategy_returns'].std() + 1e-9) * np.sqrt(252 * 96) # M15 -> ~96 bars/day\n",
    "        max_dd = self.data['drawdown'].min()\n",
    "        \n",
    "        return {\n",
    "            'Strategy': self.strategy_name,\n",
    "            'Total Return': f\"{total_return:.2%}\",\n",
    "            'Sharpe Ratio': f\"{sharpe:.2f}\",\n",
    "            'Max Drawdown': f\"{max_dd:.2%}\",\n",
    "            'Final Equity': f\"{self.data['equity'].iloc[-1]:.2f}\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Strat\u00e9gie 1 : R\u00e8gles (EMA + RSI)\n",
    "**Logique** :\n",
    "- **Tendance** : EMA 50 > EMA 200 (Long) / EMA 50 < EMA 200 (Short)\n",
    "- **Entr\u00e9e** : RSI < 30 (Oversold -> Achat) / RSI > 70 (Overbought -> Vente)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_ema_rsi(df):\n",
    "    signals = pd.Series(0, index=df.index)\n",
    "    \n",
    "    # Conditions (Vectoris\u00e9)\n",
    "    # Note: Assurez-vous que les colonnes 'ema_50', 'ema_200', 'rsi_14' existent (cr\u00e9\u00e9es dans T05)\n",
    "    # Si ema_200 n'est pas l\u00e0, on utilisera ema_20 vs ema_50\n",
    "    \n",
    "    # Utilisation des colonnes existantes (d'apr\u00e8s run_features_T05.py : ema_20, ema_50, rsi_14)\n",
    "    \n",
    "    trend_bull = df['ema_20'] > df['ema_50']\n",
    "    trend_bear = df['ema_20'] < df['ema_50']\n",
    "    \n",
    "    long_entry = trend_bull & (df['rsi_14'] < 40) # Pullback en tendance haussi\u00e8re\n",
    "    short_entry = trend_bear & (df['rsi_14'] > 60) # Pullback en tendance baissi\u00e8re\n",
    "    \n",
    "    signals[long_entry] = 1\n",
    "    signals[short_entry] = -1\n",
    "    \n",
    "    # Forward fill (Hold position until signal change or exit?)\n",
    "    # Ici on simplifie: on reste en position tant que la condition est vraie puis on sort (0).\n",
    "    # Ou mieux : on garde la derni\u00e8re position (trend following)\n",
    "    signals = signals.replace(0, np.nan).fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    return signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Strat\u00e9gie 2 : Machine Learning (Random Forest)\n",
    "Entra\u00eenement sur 2022, Validation 2023 (Optimisation simul\u00e9e), Test 2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features pour le ML\n",
    "features_cols = ['rsi_14', 'ema_20', 'ema_50', 'atr_14', 'adx_14', 'return_15m', 'rolling_std_20']\n",
    "\n",
    "# Standardisation\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data[features_cols])\n",
    "y_train = train_data['target']\n",
    "\n",
    "# Entra\u00eenement\n",
    "model_rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Pr\u00e9diction sur Test (2024)\n",
    "X_test = scaler.transform(test_data[features_cols])\n",
    "obs_pred = model_rf.predict(X_test)\n",
    "\n",
    "# Conversion 0/1 -> -1/1 (0 -> Short, 1 -> Long)\n",
    "signals_ml = pd.Series(np.where(obs_pred == 1, 1, -1), index=test_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ex\u00e9cution des Backtests sur 2024 (Test Set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "# 1. R\u00e8gles EMA+RSI\n",
    "signals_rules = strategy_ema_rsi(test_data)\n",
    "bt_rules = Backtester(test_data, \"EMA + RSI\")\n",
    "equity_rules = bt_rules.run(signals_rules)\n",
    "results_list.append(bt_rules.metrics())\n",
    "\n",
    "# 2. Machine Learning RF\n",
    "bt_ml = Backtester(test_data, \"Machine Learning (RF)\")\n",
    "equity_ml = bt_ml.run(signals_ml)\n",
    "results_list.append(bt_ml.metrics())\n",
    "\n",
    "# 3. Baseline / RL (Simul\u00e9e)\n",
    "# Comme le mod\u00e8le RL n'est pas entra\u00eenable ici (pas de stable-baselines3), on utilise le Buy & Hold comme baseline de r\u00e9f\u00e9rence ultime ou une strat\u00e9gie al\u00e9atoire pour illustrer.\n",
    "# Note: Dans un environnement complet, on chargerait l'agent PPO ici.\n",
    "signals_bh = pd.Series(1, index=test_data.index) # Buy & Hold\n",
    "bt_bh = Backtester(test_data, \"Buy & Hold (Baseline)\")\n",
    "equity_bh = bt_bh.run(signals_bh)\n",
    "results_list.append(bt_bh.metrics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparaison et Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results_list)\n",
    "print(\"### Tableau Comparatif (Test 2024) ###\")\n",
    "display(results_df)\n",
    "\n",
    "# Plot Equity Curves\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(equity_rules, label='EMA + RSI (R\u00e8gles)', color='#4E8D7C', alpha=0.8)\n",
    "plt.plot(equity_ml, label='Random Forest (ML)', color='#E85A4F', alpha=0.8)\n",
    "plt.plot(equity_bh, label='Buy & Hold', color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.title(\"Comparaison des Strat\u00e9gies sur 2024 (Test Set)\", fontsize=14)\n",
    "plt.ylabel(\"Capital ($)\")\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig('../artifacts/T09_comparative_equity_2024.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Conclusion\n",
    "\n",
    "**Analyse des R\u00e9sultats** :\n",
    "- La strat\u00e9gie **Rules-Based (EMA+RSI)** offre souvent un meilleur contr\u00f4le du risque (Drawdown) mais peut sous-performer en march\u00e9 range.\n",
    "- Le **Machine Learning (RF)** tente de capturer des patrons non-lin\u00e9aires. S'il surperforme le Buy & Hold en 2024, c'est un signe fort de robustesse.\n",
    "- Le **Buy & Hold** sert de r\u00e9f\u00e9rence de march\u00e9. Si les strat\u00e9gies actives font moins bien que le Buy & Hold, elles ne valent pas le risque/co\u00fbt.\n",
    "\n",
    "**Note sur le RL** : L'environnement `TradingEnv` a \u00e9t\u00e9 d\u00e9fini et test\u00e9 dans T08, mais l'entra\u00eenement d'un agent PPO performant n\u00e9cessite des ressources de calcul et des librairies sp\u00e9cifiques (stable-baselines3) non disponibles ici. Pour une impl\u00e9mentation future, l'agent RL utiliserait les m\u00eames observations que le RF mais optimiserait directement la Reward (PnL ajust\u00e9 du risque) plutot que la pr\u00e9cision directionnelle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. T11: Versioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Classe `ModelRegistry`\n",
    "Le c\u0153ur du syst\u00e8me de versioning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelRegistry:\n",
    "    def __init__(self, registry_path=MODELS_DIR):\n",
    "        self.registry_path = registry_path\n",
    "        self.registry_file = os.path.join(registry_path, 'registry.json')\n",
    "        self._load_registry()\n",
    "\n",
    "    def _load_registry(self):\n",
    "        if os.path.exists(self.registry_file):\n",
    "            with open(self.registry_file, 'r') as f:\n",
    "                self.registry = json.load(f)\n",
    "        else:\n",
    "            self.registry = {'models': {}}\n",
    "\n",
    "    def _save_registry(self):\n",
    "        with open(self.registry_file, 'w') as f:\n",
    "            json.dump(self.registry, f, indent=4)\n",
    "\n",
    "    def register_model(self, model, name, params, metrics, author=\"User\"):\n",
    "        \"\"\"Enregistre une nouvelle version du mod\u00e8le.\"\"\"\n",
    "        if name not in self.registry['models']:\n",
    "            self.registry['models'][name] = []\n",
    "        \n",
    "        version_id = len(self.registry['models'][name]) + 1\n",
    "        version_tag = f\"v{version_id}\"\n",
    "        timestamp = datetime.now().isoformat()\n",
    "        \n",
    "        # Cr\u00e9ation du dossier de version\n",
    "        model_dir = os.path.join(self.registry_path, name, version_tag)\n",
    "        os.makedirs(model_dir, exist_ok=True)\n",
    "        \n",
    "        # Sauvegarde du mod\u00e8le (Pickle)\n",
    "        model_path = os.path.join(model_dir, 'model.pkl')\n",
    "        with open(model_path, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "            \n",
    "        # M\u00e9tadonn\u00e9es\n",
    "        metadata = {\n",
    "            'version': version_tag,\n",
    "            'timestamp': timestamp,\n",
    "            'author': author,\n",
    "            'params': params,\n",
    "            'metrics': metrics,\n",
    "            'path': model_path\n",
    "        }\n",
    "        \n",
    "        # Sauvegarde locale des m\u00e9tadonn\u00e9es\n",
    "        with open(os.path.join(model_dir, 'meta.json'), 'w') as f:\n",
    "            json.dump(metadata, f, indent=4)\n",
    "            \n",
    "        # Mise \u00e0 jour du registre central\n",
    "        self.registry['models'][name].append(metadata)\n",
    "        self._save_registry()\n",
    "        \n",
    "        print(f\"\u2705 Mod\u00e8le {name} version {version_tag} enregistr\u00e9 avec succ\u00e8s.\")\n",
    "        return version_tag\n",
    "\n",
    "    def get_history(self, name):\n",
    "        \"\"\"Retourne l'historique des versions sous forme de DataFrame.\"\"\"\n",
    "        if name not in self.registry['models']:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        history = []\n",
    "        for entry in self.registry['models'][name]:\n",
    "            # Aplatir le dictionnaire pour le DataFrame\n",
    "            row = {\n",
    "                'version': entry['version'],\n",
    "                'date': entry['timestamp'][:10],\n",
    "                **entry['metrics'], # Metriques en colonnes\n",
    "                **{f\"param_{k}\": v for k, v in entry['params'].items()} # Params pr\u00e9fix\u00e9s\n",
    "            }\n",
    "            history.append(row)\n",
    "            \n",
    "        return pd.DataFrame(history)\n",
    "\n",
    "    def load_model(self, name, version='latest'):\n",
    "        \"\"\"Charge un mod\u00e8le sp\u00e9cifique ou le dernier.\"\"\"\n",
    "        if name not in self.registry['models']:\n",
    "            raise ValueError(f\"Mod\u00e8le {name} inconnu.\")\n",
    "            \n",
    "        if version == 'latest':\n",
    "            meta = self.registry['models'][name][-1]\n",
    "        else:\n",
    "            meta = next((m for m in self.registry['models'][name] if m['version'] == version), None)\n",
    "            if meta is None:\n",
    "                raise ValueError(f\"Version {version} non trouv\u00e9e pour {name}.\")\n",
    "        \n",
    "        with open(meta['path'], 'rb') as f:\n",
    "            model = pickle.load(f)\n",
    "            \n",
    "        print(f\"\ud83d\udcc2 Mod\u00e8le {name} ({meta['version']}) charg\u00e9.\")\n",
    "        return model, meta\n",
    "\n",
    "    def compare_versions(self, name, v_a, v_b):\n",
    "        \"\"\"G\u00e9n\u00e8re un rapport de comparaison entre deux versions.\"\"\"\n",
    "        df = self.get_history(name)\n",
    "        row_a = df[df['version'] == v_a].iloc[0]\n",
    "        row_b = df[df['version'] == v_b].iloc[0]\n",
    "        \n",
    "        print(f\"--- Comparaison : {v_a} vs {v_b} ---n\")\n",
    "        \n",
    "        # Comparaison M\u00e9triques\n",
    "        metrics_cols = [c for c in df.columns if c not in ['version', 'date'] and not c.startswith('param_')]\n",
    "        diffs = []\n",
    "        for m in metrics_cols:\n",
    "            val_a = row_a[m]\n",
    "            val_b = row_b[m]\n",
    "            diff = val_b - val_a\n",
    "            pct = (diff / val_a) * 100 if val_a != 0 else 0\n",
    "            icon = \"\ud83d\udfe2\" if diff > 0 else \"\ud83d\udd34\" if diff < 0 else \"\u26aa\"\n",
    "            diffs.append({'M\u00e9trique': m, v_a: val_a, v_b: val_b, 'Diff': diff, 'Diff %': f\"{pct:+.2f}%\", 'Status': icon})\n",
    "            \n",
    "        print(\"\\nM\u00e9triques :\")\n",
    "        display(pd.DataFrame(diffs))\n",
    "        \n",
    "        # Comparaison Param\u00e8tres\n",
    "        params_cols = [c for c in df.columns if c.startswith('param_')]\n",
    "        param_changes = []\n",
    "        for p in params_cols:\n",
    "            if row_a[p] != row_b[p]:\n",
    "                param_changes.append({'Param\u00e8tre': p.replace('param_', ''), v_a: row_a[p], v_b: row_b[p]})\n",
    "        \n",
    "        if param_changes:\n",
    "            print(\"\\nChangements de Param\u00e8tres :\")\n",
    "            display(pd.DataFrame(param_changes))\n",
    "        else:\n",
    "            print(\"\\nAucun changement de param\u00e8tre d\u00e9tect\u00e9.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn\u00e9es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_dir):\n",
    "    files = ['GBPUSD_M15_2022_features.csv', 'GBPUSD_M15_2024_features.csv'] # On charge Train et Test\n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        path = os.path.join(data_dir, f)\n",
    "        if os.path.exists(path):\n",
    "            df_year = pd.read_csv(path, parse_dates=['timestamp'], index_col='timestamp')\n",
    "            dfs.append(df_year)\n",
    "            \n",
    "    if not dfs:\n",
    "        raise FileNotFoundError(\"Donn\u00e9es non trouv\u00e9es.\")\n",
    "        \n",
    "    df = pd.concat(dfs)\n",
    "    df.sort_index(inplace=True)\n",
    "    \n",
    "    # Target\n",
    "    df['target_return'] = df['close_15m'].shift(-1) - df['close_15m']\n",
    "    df['target'] = (df['target_return'] > 0).astype(int)\n",
    "    df.dropna(inplace=True)\n",
    "    return df\n",
    "\n",
    "df = load_data(DATA_DIR)\n",
    "train_data = df.loc['2022']\n",
    "test_data = df.loc['2024']\n",
    "\n",
    "features = ['rsi_14', 'ema_20', 'ema_50', 'atr_14', 'adx_14']\n",
    "X_train, y_train = train_data[features], train_data['target']\n",
    "X_test, y_test = test_data[features], test_data['target']\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Entra\u00eenement et Versioning\n",
    "\n",
    "### Version 1 : Baseline (Random Forest Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = ModelRegistry()\n",
    "MODEL_NAME = \"rf_direction_classifier\"\n",
    "\n",
    "# --- V1 ---\n",
    "params_v1 = {\n",
    "    'n_estimators': 50,\n",
    "    'max_depth': 3,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Training V1...\")\n",
    "model_v1 = RandomForestClassifier(**params_v1)\n",
    "model_v1.fit(X_train, y_train)\n",
    "\n",
    "# Eval V1\n",
    "y_pred_v1 = model_v1.predict(X_test)\n",
    "metrics_v1 = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_v1),\n",
    "    'f1_score': f1_score(y_test, y_pred_v1)\n",
    "}\n",
    "\n",
    "# Enregistrement V1\n",
    "registry.register_model(model_v1, MODEL_NAME, params_v1, metrics_v1, author=\"JCLoirat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Version 2 : Optimis\u00e9 (Plus d'arbres, plus profond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- V2 ---\n",
    "params_v2 = {\n",
    "    'n_estimators': 200,  # Augment\u00e9\n",
    "    'max_depth': 10,      # Augment\u00e9\n",
    "    'min_samples_leaf': 5, # Ajout\u00e9 pour \u00e9viter l'overfit\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "print(\"Training V2...\")\n",
    "model_v2 = RandomForestClassifier(**params_v2)\n",
    "model_v2.fit(X_train, y_train)\n",
    "\n",
    "# Eval V2\n",
    "y_pred_v2 = model_v2.predict(X_test)\n",
    "metrics_v2 = {\n",
    "    'accuracy': accuracy_score(y_test, y_pred_v2),\n",
    "    'f1_score': f1_score(y_test, y_pred_v2)\n",
    "}\n",
    "\n",
    "# Enregistrement V2\n",
    "registry.register_model(model_v2, MODEL_NAME, params_v2, metrics_v2, author=\"JCLoirat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse et Rapport d'\u00c9volution\n",
    "Visualisation de l'historique et des changements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher tout l'historique\n",
    "print(\"\ud83d\udcdc Historique du Mod\u00e8le :\")\n",
    "history = registry.get_history(MODEL_NAME)\n",
    "display(history)\n",
    "\n",
    "# Comparer V1 et V2\n",
    "registry.compare_versions(MODEL_NAME, 'v1', 'v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. D\u00e9mo : Chargement en Production\n",
    "Simulation de l'utilisation du registry par une API ou un syst\u00e8me de trading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\ud83d\ude80 Simulation Production Startup...\")\n",
    "\n",
    "# Chargement automatique de la derni\u00e8re version\n",
    "prod_model, prod_meta = registry.load_model(MODEL_NAME, version='latest')\n",
    "\n",
    "print(f\"Pr\u00eat \u00e0 utiliser le mod\u00e8le v{prod_meta['version']} cr\u00e9\u00e9 le {prod_meta['timestamp']}\")\n",
    "print(f\"Performance attendue (Accuracy) : {prod_meta['metrics']['accuracy']:.2%}\")\n",
    "\n",
    "# Test inf\u00e9rence rapide\n",
    "sample = X_test.iloc[0:1]\n",
    "prediction = prod_model.predict(sample)\n",
    "print(f\"Prediction pour l'\u00e9chantillon : {'HAUSSE' if prediction[0]==1 else 'BAISSE'}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}