{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Master Trading Notebook: GBP/USD M15 Project\n",
                "\n",
                "## Context and Objectives\n",
                "\n",
                "This master notebook consolidates the entire workflow of the GBP/USD trading project, from raw data ingestion to machine learning model evaluation. It unifies the following steps:\n",
                "\n",
                "1.  **Data Import (T01)**: Loading and auditing raw M1 data (2022-2024).\n",
                "2.  **Aggregation (T02)**: Transforming M1 ticks into 15-minute (M15) candles.\n",
                "3.  **Cleaning (T03)**: Filtering incomplete candles and ensuring data integrity.\n",
                "4.  **Feature Engineering (T05)**: Creating technical indicators (RSI, EMAs, ATR, etc.).\n",
                "5.  **Machine Learning (T07)**: Training, validating, and backtesting predictive models.\n",
                "\n",
                "---\n",
                "\n",
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import os\n",
                "import sys\n",
                "from pathlib import Path\n",
                "from scipy import stats as sp_stats\n",
                "from statsmodels.tsa.stattools import adfuller\n",
                "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
                "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
                "import pickle\n",
                "import json\n",
                "from datetime import datetime\n",
                "\n",
                "# Graphics Configuration\n",
                "sns.set_theme(style=\"whitegrid\")\n",
                "plt.rcParams.update({\n",
                "    \"figure.facecolor\": \"#FAF0E6\",\n",
                "    \"axes.facecolor\": \"#F5F5DC\",\n",
                "    \"grid.color\": \"#E0D0C0\",\n",
                "    \"text.color\": \"#5D4037\",\n",
                "    \"axes.labelcolor\": \"#5D4037\",\n",
                "    \"xtick.color\": \"#5D4037\",\n",
                "    \"ytick.color\": \"#5D4037\",\n",
                "    \"axes.prop_cycle\": plt.cycler(color=['#8D6E63', '#A1887F', '#D7CCC8'])\n",
                "})\n",
                "\n",
                "# Constants and Paths\n",
                "PROJECT_ROOT = Path('.').resolve()\n",
                "DATA_DIR = PROJECT_ROOT / \"data\"\n",
                "M15_DIR = DATA_DIR / \"m15\"\n",
                "CLEAN_DIR = M15_DIR / \"clean\"\n",
                "FEATURES_DIR = DATA_DIR / \"features\"\n",
                "MODELS_DIR = PROJECT_ROOT / \"models\" / \"v1\"\n",
                "\n",
                "for d in [DATA_DIR, M15_DIR, CLEAN_DIR, FEATURES_DIR, MODELS_DIR]:\n",
                "    d.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "YEARS = [2022, 2023, 2024]\n",
                "LABELS = {2022: 'Train', 2023: 'Validation', 2024: 'Test'}\n",
                "COLORS = {2022: '#8D6E63', 2023: '#A1887F', 2024: '#BCAAA4'}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. T01: Import M1 Data\n",
                "Loading raw 1-minute data from CSV files."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def load_m1_data(year):\n",
                "    # Attempt to locate the file in probable locations\n",
                "    filename = f\"DAT_MT_GBPUSD_M1_{year}.csv\"\n",
                "    possible_paths = [\n",
                "        DATA_DIR / filename,\n",
                "        DATA_DIR / f\"HISTDATA_COM_MT_GBPUSD_M1{year}\" / filename,\n",
                "        PROJECT_ROOT / f\"HISTDATA_COM_MT_GBPUSD_M1{year}\" / filename\n",
                "    ]\n",
                "    \n",
                "    path = None\n",
                "    for p in possible_paths:\n",
                "        if p.exists():\n",
                "            path = p\n",
                "            break\n",
                "            \n",
                "    if path is None:\n",
                "        print(f\"[ERROR] File for {year} not found.\")\n",
                "        return None\n",
                "        \n",
                "    print(f\"Loading {year} from {path}...\")\n",
                "    # Standard M1 CSV format: Date, Time, Open, High, Low, Close, Volume\n",
                "    df = pd.read_csv(path, names=['date', 'time', 'open', 'high', 'low', 'close', 'volume'])\n",
                "    \n",
                "    # Timestamp creation\n",
                "    df['timestamp'] = pd.to_datetime(df['date'] + ' ' + df['time'])\n",
                "    df.set_index('timestamp', inplace=True)\n",
                "    df.drop(['date', 'time'], axis=1, inplace=True)\n",
                "    \n",
                "    return df\n",
                "\n",
                "dfs_m1 = {}\n",
                "for year in YEARS:\n",
                "    df = load_m1_data(year)\n",
                "    if df is not None:\n",
                "        dfs_m1[year] = df\n",
                "        print(f\" -> Loaded {len(df):,} M1 rows.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. T02: Aggregation M1 -> M15\n",
                "Converting high-frequency 1-minute data into 15-minute candles to reduce noise and align with the trading strategy."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def aggregate_m1_to_m15(df_m1):\n",
                "    if df_m1 is None or df_m1.empty:\n",
                "        return None\n",
                "        \n",
                "    # Resampling rules\n",
                "    # Open: first, High: max, Low: min, Close: last, Volume: sum\n",
                "    # Tick count: count of M1 bars contributing to the M15 bar\n",
                "    agg_dict = {\n",
                "        'open': 'first',\n",
                "        'high': 'max',\n",
                "        'low': 'min',\n",
                "        'close': 'last',\n",
                "        'volume': 'sum'\n",
                "    }\n",
                "    \n",
                "    # Count ticks before full aggregation to ensure alignment\n",
                "    ticks = df_m1['close'].resample('15T').count()\n",
                "    \n",
                "    # Main Aggregation\n",
                "    df_m15 = df_m1.resample('15T').agg(agg_dict)\n",
                "    df_m15['tick_count'] = ticks\n",
                "    \n",
                "    # Drop rows where no data exists (e.g. gaps in resampling)\n",
                "    df_m15.dropna(inplace=True)\n",
                "    \n",
                "    # Rename columns\n",
                "    df_m15.rename(columns={\n",
                "        'open': 'open_15m',\n",
                "        'high': 'high_15m',\n",
                "        'low': 'low_15m',\n",
                "        'close': 'close_15m'\n",
                "    }, inplace=True)\n",
                "    \n",
                "    return df_m15\n",
                "\n",
                "dfs_m15 = {}\n",
                "for year in YEARS:\n",
                "    if year in dfs_m1:\n",
                "        print(f\"Aggregating {year}...\")\n",
                "        df_m15 = aggregate_m1_to_m15(dfs_m1[year])\n",
                "        dfs_m15[year] = df_m15\n",
                "        \n",
                "        # Save raw M15\n",
                "        out_path = M15_DIR / f\"GBPUSD_M15_{year}.csv\"\n",
                "        df_m15.to_csv(out_path)\n",
                "        print(f\" -> Created {len(df_m15):,} M15 candles. Saved to {out_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. T03: Cleaning Data\n",
                "Filtering low-liquidity candles (tick_count < 5) and checking for OHLC consistency."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def clean_data(df, year):\n",
                "    init_len = len(df)\n",
                "    \n",
                "    # 1. Filter incomplete candles\n",
                "    # A proper 15m candle should have at least 5 minutes of activity\n",
                "    df_clean = df[df['tick_count'] >= 5].copy()\n",
                "    dropped_ticks = init_len - len(df_clean)\n",
                "    \n",
                "    # 2. OHLC Consistency Check\n",
                "    mask_coherence = df_clean['high_15m'] >= df_clean['low_15m']\n",
                "    df_clean = df_clean[mask_coherence]\n",
                "    dropped_coherence = (init_len - dropped_ticks) - len(df_clean)\n",
                "\n",
                "    print(f\"[{year}] Cleaned. Dropped: {dropped_ticks} (low ticks), {dropped_coherence} (incoherent)\")\n",
                "    return df_clean\n",
                "\n",
                "cleaned_dfs = {}\n",
                "for year in YEARS:\n",
                "    if year in dfs_m15:\n",
                "        df_clean = clean_data(dfs_m15[year], year)\n",
                "        cleaned_dfs[year] = df_clean\n",
                "        \n",
                "        out_path = CLEAN_DIR / f\"GBPUSD_M15_{year}_clean.csv\"\n",
                "        df_clean.to_csv(out_path)\n",
                "        print(f\" -> Saved clean data to {out_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. T05: Feature Engineering\n",
                "Adding technical indicators: RSI, ATR, ADX, EMAs, Returns."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# --- Indicator Functions ---\n",
                "\n",
                "def calculate_rsi(series, period=14):\n",
                "    delta = series.diff()\n",
                "    gain = (delta.where(delta > 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
                "    loss = (-delta.where(delta < 0, 0)).ewm(alpha=1/period, adjust=False).mean()\n",
                "    rs = gain / loss\n",
                "    return 100 - (100 / (1 + rs))\n",
                "\n",
                "def calculate_atr(df, period=14):\n",
                "    high = df['high_15m']\n",
                "    low = df['low_15m']\n",
                "    close = df['close_15m']\n",
                "    prev_close = close.shift(1)\n",
                "    \n",
                "    tr1 = high - low\n",
                "    tr2 = (high - prev_close).abs()\n",
                "    tr3 = (low - prev_close).abs()\n",
                "    \n",
                "    tr = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
                "    atr = tr.ewm(alpha=1/period, adjust=False).mean()\n",
                "    return atr\n",
                "\n",
                "def calculate_adx(df, period=14):\n",
                "    high = df['high_15m']\n",
                "    low = df['low_15m']\n",
                "    close = df['close_15m']\n",
                "    \n",
                "    plus_dm = high.diff()\n",
                "    minus_dm = low.diff()\n",
                "    plus_dm[plus_dm < 0] = 0\n",
                "    minus_dm[minus_dm > 0] = 0\n",
                "    \n",
                "    atr = calculate_atr(df, period)\n",
                "    \n",
                "    plus_di = 100 * (plus_dm.ewm(alpha=1/period).mean() / atr)\n",
                "    minus_di = 100 * (minus_dm.abs().ewm(alpha=1/period).mean() / atr)\n",
                "    \n",
                "    dx = (abs(plus_di - minus_di) / (plus_di + minus_di)) * 100\n",
                "    adx = dx.ewm(alpha=1/period).mean()\n",
                "    return adx\n",
                "\n",
                "def calculate_macd(series, fast=12, slow=26, signal=9):\n",
                "    ema_fast = series.ewm(span=fast, adjust=False).mean()\n",
                "    ema_slow = series.ewm(span=slow, adjust=False).mean()\n",
                "    macd_line = ema_fast - ema_slow\n",
                "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
                "    return macd_line, signal_line\n",
                "\n",
                "def generate_features(df):\n",
                "    df_feat = df.copy()\n",
                "    close = df_feat['close_15m']\n",
                "    \n",
                "    # Returns\n",
                "    df_feat['return_1'] = close.pct_change(1)\n",
                "    df_feat['return_4'] = close.pct_change(4)\n",
                "    \n",
                "    # EMAs\n",
                "    df_feat['ema_20'] = close.ewm(span=20, adjust=False).mean()\n",
                "    df_feat['ema_50'] = close.ewm(span=50, adjust=False).mean()\n",
                "    df_feat['ema_diff'] = df_feat['ema_20'] - df_feat['ema_50']\n",
                "    \n",
                "    # RSI\n",
                "    df_feat['rsi_14'] = calculate_rsi(close, 14)\n",
                "    \n",
                "    # Volatility\n",
                "    df_feat['rolling_std_20'] = close.rolling(window=20).std()\n",
                "    \n",
                "    # Candle Features\n",
                "    df_feat['range_15m'] = df_feat['high_15m'] - df_feat['low_15m']\n",
                "    df_feat['body'] = (df_feat['close_15m'] - df_feat['open_15m']).abs()\n",
                "    df_feat['upper_wick'] = df_feat['high_15m'] - df_feat[['open_15m', 'close_15m']].max(axis=1)\n",
                "    df_feat['lower_wick'] = df_feat[['open_15m', 'close_15m']].min(axis=1) - df_feat['low_15m']\n",
                "    \n",
                "    # Trend / Context\n",
                "    df_feat['ema_200'] = close.ewm(span=200, adjust=False).mean()\n",
                "    df_feat['distance_to_ema200'] = close - df_feat['ema_200']\n",
                "    df_feat['slope_ema50'] = df_feat['ema_50'].diff(3)\n",
                "    \n",
                "    df_feat['atr_14'] = calculate_atr(df_feat, 14)\n",
                "    df_feat['rolling_std_100'] = close.rolling(window=100).std()\n",
                "    df_feat['volatility_ratio'] = df_feat['rolling_std_20'] / df_feat['rolling_std_100']\n",
                "    \n",
                "    df_feat['adx_14'] = calculate_adx(df_feat, 14)\n",
                "    df_feat['macd'], df_feat['macd_signal'] = calculate_macd(close)\n",
                "    \n",
                "    # Drop warmup NaNs\n",
                "    df_feat.dropna(inplace=True)\n",
                "    \n",
                "    return df_feat"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "feature_sets = {}\n",
                "for year in YEARS:\n",
                "    if year in cleaned_dfs:\n",
                "        print(f\"Generating features for {year}...\")\n",
                "        df_feat = generate_features(cleaned_dfs[year])\n",
                "        feature_sets[year] = df_feat\n",
                "        \n",
                "        out_path = FEATURES_DIR / f\"GBPUSD_M15_{year}_features.csv\"\n",
                "        df_feat.to_csv(out_path)\n",
                "        print(f\" -> Saved {len(df_feat)} rows to {out_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. T07: Machine Learning\n",
                "Training and evaluating a Random Forest model on the prepared data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def prepare_modeling_data(df):\n",
                "    # Target: Next Close > Current Close\n",
                "    df = df.copy()\n",
                "    df['target_return'] = df['close_15m'].shift(-1) - df['close_15m']\n",
                "    df['target'] = (df['target_return'] > 0).astype(int)\n",
                "    df.dropna(inplace=True)\n",
                "    return df\n",
                "\n",
                "if 2022 in feature_sets and 2023 in feature_sets and 2024 in feature_sets:\n",
                "    df_train = prepare_modeling_data(feature_sets[2022])\n",
                "    df_val = prepare_modeling_data(feature_sets[2023])\n",
                "    df_test = prepare_modeling_data(feature_sets[2024])\n",
                "    \n",
                "    # Feature selection\n",
                "    drop_cols = ['target', 'target_return', 'open_15m', 'high_15m', 'low_15m', 'close_15m', 'volume', 'volume_15m', 'tick_count']\n",
                "    features_cols = [c for c in df_train.columns if c not in drop_cols]\n",
                "    \n",
                "    X_train = df_train[features_cols]\n",
                "    y_train = df_train['target']\n",
                "    \n",
                "    X_val = df_val[features_cols]\n",
                "    y_val = df_val['target']\n",
                "    \n",
                "    X_test = df_test[features_cols]\n",
                "    y_test = df_test['target']\n",
                "    \n",
                "    # Scaling\n",
                "    scaler = StandardScaler()\n",
                "    X_train_scaled = scaler.fit_transform(X_train)\n",
                "    X_val_scaled = scaler.transform(X_val)\n",
                "    X_test_scaled = scaler.transform(X_test)\n",
                "    \n",
                "    print(f\"Training Data Shape: {X_train.shape}\")\n",
                "    \n",
                "    # Model Training\n",
                "    model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42, class_weight='balanced')\n",
                "    print(\"Training Random Forest...\")\n",
                "    model.fit(X_train_scaled, y_train)\n",
                "    \n",
                "    # Evaluation\n",
                "    val_pred = model.predict(X_val_scaled)\n",
                "    acc_val = accuracy_score(y_val, val_pred)\n",
                "    print(f\"Validation Accuracy: {acc_val:.4f}\")\n",
                "    print(classification_report(y_val, val_pred))\n",
                "    \n",
                "    # Save Model\n",
                "    with open(MODELS_DIR / \"rf_model.pkl\", \"wb\") as f:\n",
                "        pickle.dump(model, f)\n",
                "    with open(MODELS_DIR / \"scaler.pkl\", \"wb\") as f:\n",
                "        pickle.dump(scaler, f)\n",
                "    print(\"Model saved.\")\n",
                "    \n",
                "    # Backtest Visualization\n",
                "    preds_test = model.predict(X_test_scaled)\n",
                "    signals = np.where(preds_test == 1, 1, -1)\n",
                "    strategy_returns = signals * df_test['target_return']\n",
                "    cumulative_returns = strategy_returns.cumsum()\n",
                "    \n",
                "    plt.figure(figsize=(12, 6))\n",
                "    plt.plot(cumulative_returns.index, cumulative_returns, label='Strategy (RF)', color='#8D6E63')\n",
                "    plt.plot(df_test.index, df_test['target_return'].cumsum(), label='Buy & Hold', color='gray', alpha=0.5, linestyle='--')\n",
                "    plt.title(\"Backtest Result: 2024 Test Set\", fontweight='bold')\n",
                "    plt.ylabel(\"Cumulative PnL\")\n",
                "    plt.legend()\n",
                "    plt.show()\n",
                "else:\n",
                "    print(\"Can't proceed with ML: Missing data for one of the years.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.10"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
